# Evaluation Report: Distributed System API Gateway

**Date:** 2025-10-12
**Run Label:** baseline

## 1. Experimental Setup
- **Hardware environment:** 2× AMD EPYC 7742 (64 cores each), 512 GB RAM, 2× NVIDIA RTX 4090 GPUs, 10 Gbps NIC
- **Operating system & versions:** Ubuntu 22.04, Docker 27.1.2, docker-compose 2.29.2, Python 3.10.14
- **Number of containerized nodes:** gateway=1, scheduler=1, worker=4, database=1
- **System under test:** HTTP API gateway at `http://localhost:8080/submit`
- **Workload specifications:**
  - Warmup requests: 50
  - Requests per level: 500
  - Concurrency levels: [1, 2, 4, 8, 16, 32, 64]
  - Timeout per request: 30s
  - Payload: `{'artifact': 'demo_model_v1'}`

> Notes: Replace placeholders above with your exact machine types, CPU/GPU counts, memory, network, container image versions, and orchestration details. If you scale the number of service replicas between runs, list each run label and replica counts.

## 2. Performance & Scalability Results

### 2.1 Throughput vs Concurrency
![Throughput](./baseline_throughput.png)

### 2.2 p95 Latency vs Concurrency
![p95 Latency](./baseline_p95_latency.png)

### 2.3 Error Rate vs Concurrency
![Error Rate](./baseline_error_rate.png)

### 2.4 Summary Table
See `baseline_summary_clean.csv` for the exact numbers (throughput, latency percentiles, error rates).

## 3. Analysis of System-Design Trade-offs

- **Backpressure & Queueing:** Discuss how the gateway or downstream services handle bursts as concurrency increases. Note at which concurrency the p95 latency accelerates and error rate starts to rise.
- **Scalability:** Compare runs at different replica counts (e.g., 1×, 2×, 4× service instances). Verify whether throughput scales close to linearly and whether latency remains acceptable.
- **Fault tolerance:** Summarize error patterns (HTTP status codes, timeouts). Identify whether failures cluster under high concurrency or specific payloads.
- **Bottlenecks:** Hypothesize where hotspots occur (gateway CPU, app threads, DB, model server, network I/O). Support with evidence if you can collect node-level metrics.
- **Cost-performance:** If you have per-node specs/costs, estimate cost per 1000 successful requests at target SLOs (e.g., p95 < 200 ms, error rate < 1%).

## 4. Conclusions & Recommendations

- **Target operating point:** Choose a concurrency where throughput is high and p95 latency/error rate meet SLOs.
- **Capacity planning:** Given observed scaling, estimate how many replicas are required for your production RPS targets.
- **Next steps:** E.g., enable connection pooling, increase worker processes, add caching, tune timeouts/retries, or shard queues.

---

*This report was generated by the included scripts (`benchmark.py`, `analyze.py`, `generate_report.py`).*
